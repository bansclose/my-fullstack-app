
==> Audit <==
|--------------|-----------------|----------|---------|---------|---------------------|---------------------|
|   Command    |      Args       | Profile  |  User   | Version |     Start Time      |      End Time       |
|--------------|-----------------|----------|---------|---------|---------------------|---------------------|
| start        |                 | minikube | trungnt | v1.35.0 | 25 Feb 25 10:22 +07 |                     |
| start        |                 | minikube | trungnt | v1.35.0 | 25 Feb 25 10:24 +07 | 25 Feb 25 10:26 +07 |
| update-check |                 | minikube | trungnt | v1.35.0 | 25 Feb 25 16:37 +07 | 25 Feb 25 16:37 +07 |
| update-check |                 | minikube | trungnt | v1.35.0 | 26 Feb 25 13:21 +07 | 26 Feb 25 13:21 +07 |
| start        |                 | minikube | trungnt | v1.35.0 | 26 Feb 25 20:14 +07 | 26 Feb 25 20:14 +07 |
| addons       | enable ingress  | minikube | trungnt | v1.35.0 | 26 Feb 25 20:19 +07 | 26 Feb 25 20:20 +07 |
| addons       | enable ingress  | minikube | trungnt | v1.35.0 | 26 Feb 25 20:48 +07 | 26 Feb 25 20:48 +07 |
| addons       | list            | minikube | trungnt | v1.35.0 | 26 Feb 25 20:49 +07 | 26 Feb 25 20:49 +07 |
| addons       | disable ingress | minikube | trungnt | v1.35.0 | 26 Feb 25 20:51 +07 | 26 Feb 25 20:51 +07 |
| addons       | enable ingress  | minikube | trungnt | v1.35.0 | 26 Feb 25 20:51 +07 | 26 Feb 25 20:51 +07 |
|--------------|-----------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/26 20:14:16
Running on machine: trungnt
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0226 20:14:16.552162   17709 out.go:345] Setting OutFile to fd 1 ...
I0226 20:14:16.552319   17709 out.go:397] isatty.IsTerminal(1) = true
I0226 20:14:16.552325   17709 out.go:358] Setting ErrFile to fd 2...
I0226 20:14:16.552337   17709 out.go:397] isatty.IsTerminal(2) = true
I0226 20:14:16.552629   17709 root.go:338] Updating PATH: /home/trungnt/.minikube/bin
W0226 20:14:16.552791   17709 root.go:314] Error reading config file at /home/trungnt/.minikube/config/config.json: open /home/trungnt/.minikube/config/config.json: no such file or directory
I0226 20:14:16.554046   17709 out.go:352] Setting JSON to false
I0226 20:14:16.555592   17709 start.go:129] hostinfo: {"hostname":"trungnt","uptime":24871,"bootTime":1740550786,"procs":298,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.11.0-17-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"40763197-55e8-4594-9da9-511c0558c99f"}
I0226 20:14:16.555759   17709 start.go:139] virtualization: kvm host
I0226 20:14:16.559153   17709 out.go:177] üòÑ  minikube v1.35.0 on Ubuntu 24.04
I0226 20:14:16.560669   17709 notify.go:220] Checking for updates...
I0226 20:14:16.561647   17709 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 20:14:16.563149   17709 driver.go:394] Setting default libvirt URI to qemu:///system
I0226 20:14:16.591255   17709 docker.go:123] docker version: linux-28.0.0:Docker Engine - Community
I0226 20:14:16.591327   17709 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0226 20:14:16.995452   17709 info.go:266] docker info: {ID:2e35dfb6-9e2d-4834-8848-a406e98641d1 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-02-26 20:14:16.960264924 +0700 +07 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-17-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12401369088 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:trungnt Labels:[] ExperimentalBuild:false ServerVersion:28.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.0]] Warnings:<nil>}}
I0226 20:14:16.996412   17709 docker.go:318] overlay module found
I0226 20:14:17.003085   17709 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0226 20:14:17.004591   17709 start.go:297] selected driver: docker
I0226 20:14:17.004611   17709 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/trungnt:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 20:14:17.004813   17709 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0226 20:14:17.005063   17709 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0226 20:14:17.177703   17709 info.go:266] docker info: {ID:2e35dfb6-9e2d-4834-8848-a406e98641d1 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-02-26 20:14:17.158092919 +0700 +07 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-17-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12401369088 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:trungnt Labels:[] ExperimentalBuild:false ServerVersion:28.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.0]] Warnings:<nil>}}
I0226 20:14:17.178544   17709 cni.go:84] Creating CNI manager for ""
I0226 20:14:17.179731   17709 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0226 20:14:17.179814   17709 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/trungnt:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 20:14:17.181777   17709 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0226 20:14:17.183148   17709 cache.go:121] Beginning downloading kic base image for docker with docker
I0226 20:14:17.185990   17709 out.go:177] üöú  Pulling base image v0.0.46 ...
I0226 20:14:17.187562   17709 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0226 20:14:17.189105   17709 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0226 20:14:17.189189   17709 preload.go:146] Found local preload: /home/trungnt/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0226 20:14:17.189199   17709 cache.go:56] Caching tarball of preloaded images
I0226 20:14:17.189529   17709 preload.go:172] Found /home/trungnt/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0226 20:14:17.189543   17709 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0226 20:14:17.189947   17709 profile.go:143] Saving config to /home/trungnt/.minikube/profiles/minikube/config.json ...
I0226 20:14:17.246538   17709 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0226 20:14:17.246558   17709 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0226 20:14:17.246608   17709 cache.go:227] Successfully downloaded all kic artifacts
I0226 20:14:17.246636   17709 start.go:360] acquireMachinesLock for minikube: {Name:mk9534c12908bfae27b25534f831b0e7a1366151 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 20:14:17.246791   17709 start.go:364] duration metric: took 136.567¬µs to acquireMachinesLock for "minikube"
I0226 20:14:17.246821   17709 start.go:96] Skipping create...Using existing machine configuration
I0226 20:14:17.246827   17709 fix.go:54] fixHost starting: 
I0226 20:14:17.247152   17709 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 20:14:17.291087   17709 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0226 20:14:17.291124   17709 fix.go:138] unexpected machine state, will restart: <nil>
I0226 20:14:17.295033   17709 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0226 20:14:17.296651   17709 cli_runner.go:164] Run: docker start minikube
I0226 20:14:17.807210   17709 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 20:14:17.826847   17709 kic.go:430] container "minikube" state is running.
I0226 20:14:17.827245   17709 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0226 20:14:17.847800   17709 profile.go:143] Saving config to /home/trungnt/.minikube/profiles/minikube/config.json ...
I0226 20:14:17.848096   17709 machine.go:93] provisionDockerMachine start ...
I0226 20:14:17.848182   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:17.868457   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:17.869011   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:17.869024   17709 main.go:141] libmachine: About to run SSH command:
hostname
I0226 20:14:17.869571   17709 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:50796->127.0.0.1:32768: read: connection reset by peer
I0226 20:14:21.056453   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0226 20:14:21.056488   17709 ubuntu.go:169] provisioning hostname "minikube"
I0226 20:14:21.056757   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:21.088386   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:21.088565   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:21.088578   17709 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0226 20:14:21.279800   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0226 20:14:21.279894   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:21.299659   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:21.299821   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:21.299832   17709 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0226 20:14:21.466973   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0226 20:14:21.467041   17709 ubuntu.go:175] set auth options {CertDir:/home/trungnt/.minikube CaCertPath:/home/trungnt/.minikube/certs/ca.pem CaPrivateKeyPath:/home/trungnt/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/trungnt/.minikube/machines/server.pem ServerKeyPath:/home/trungnt/.minikube/machines/server-key.pem ClientKeyPath:/home/trungnt/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/trungnt/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/trungnt/.minikube}
I0226 20:14:21.467084   17709 ubuntu.go:177] setting up certificates
I0226 20:14:21.467105   17709 provision.go:84] configureAuth start
I0226 20:14:21.467288   17709 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0226 20:14:21.507046   17709 provision.go:143] copyHostCerts
I0226 20:14:21.507277   17709 exec_runner.go:144] found /home/trungnt/.minikube/ca.pem, removing ...
I0226 20:14:21.507973   17709 exec_runner.go:203] rm: /home/trungnt/.minikube/ca.pem
I0226 20:14:21.508022   17709 exec_runner.go:151] cp: /home/trungnt/.minikube/certs/ca.pem --> /home/trungnt/.minikube/ca.pem (1078 bytes)
I0226 20:14:21.508260   17709 exec_runner.go:144] found /home/trungnt/.minikube/cert.pem, removing ...
I0226 20:14:21.508265   17709 exec_runner.go:203] rm: /home/trungnt/.minikube/cert.pem
I0226 20:14:21.508290   17709 exec_runner.go:151] cp: /home/trungnt/.minikube/certs/cert.pem --> /home/trungnt/.minikube/cert.pem (1123 bytes)
I0226 20:14:21.508491   17709 exec_runner.go:144] found /home/trungnt/.minikube/key.pem, removing ...
I0226 20:14:21.508495   17709 exec_runner.go:203] rm: /home/trungnt/.minikube/key.pem
I0226 20:14:21.508519   17709 exec_runner.go:151] cp: /home/trungnt/.minikube/certs/key.pem --> /home/trungnt/.minikube/key.pem (1679 bytes)
I0226 20:14:21.508701   17709 provision.go:117] generating server cert: /home/trungnt/.minikube/machines/server.pem ca-key=/home/trungnt/.minikube/certs/ca.pem private-key=/home/trungnt/.minikube/certs/ca-key.pem org=trungnt.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0226 20:14:21.622104   17709 provision.go:177] copyRemoteCerts
I0226 20:14:21.622651   17709 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0226 20:14:21.622710   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:21.640448   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:21.757667   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0226 20:14:21.792192   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0226 20:14:21.820715   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0226 20:14:21.850703   17709 provision.go:87] duration metric: took 383.585493ms to configureAuth
I0226 20:14:21.850717   17709 ubuntu.go:193] setting minikube options for container-runtime
I0226 20:14:21.850845   17709 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 20:14:21.850894   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:21.868812   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:21.868994   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:21.869004   17709 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0226 20:14:22.018336   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0226 20:14:22.018365   17709 ubuntu.go:71] root file system type: overlay
I0226 20:14:22.018848   17709 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0226 20:14:22.019005   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.053393   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:22.053586   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:22.053729   17709 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0226 20:14:22.252456   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0226 20:14:22.252631   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.276846   17709 main.go:141] libmachine: Using SSH client type: native
I0226 20:14:22.277031   17709 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0226 20:14:22.277061   17709 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0226 20:14:22.458420   17709 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0226 20:14:22.458436   17709 machine.go:96] duration metric: took 4.610329199s to provisionDockerMachine
I0226 20:14:22.458450   17709 start.go:293] postStartSetup for "minikube" (driver="docker")
I0226 20:14:22.458468   17709 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0226 20:14:22.458587   17709 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0226 20:14:22.458670   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.481867   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:22.610845   17709 ssh_runner.go:195] Run: cat /etc/os-release
I0226 20:14:22.616725   17709 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0226 20:14:22.616780   17709 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0226 20:14:22.616804   17709 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0226 20:14:22.616812   17709 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0226 20:14:22.616832   17709 filesync.go:126] Scanning /home/trungnt/.minikube/addons for local assets ...
I0226 20:14:22.617074   17709 filesync.go:126] Scanning /home/trungnt/.minikube/files for local assets ...
I0226 20:14:22.617238   17709 start.go:296] duration metric: took 158.778296ms for postStartSetup
I0226 20:14:22.617314   17709 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0226 20:14:22.617369   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.640990   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:22.743979   17709 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0226 20:14:22.750260   17709 fix.go:56] duration metric: took 5.503428017s for fixHost
I0226 20:14:22.750275   17709 start.go:83] releasing machines lock for "minikube", held for 5.503474337s
I0226 20:14:22.750337   17709 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0226 20:14:22.769916   17709 ssh_runner.go:195] Run: cat /version.json
I0226 20:14:22.769966   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.770042   17709 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0226 20:14:22.770134   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:22.790841   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:22.793440   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:23.249939   17709 ssh_runner.go:195] Run: systemctl --version
I0226 20:14:23.270451   17709 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0226 20:14:23.277611   17709 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0226 20:14:23.302344   17709 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0226 20:14:23.302418   17709 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0226 20:14:23.313405   17709 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0226 20:14:23.313417   17709 start.go:495] detecting cgroup driver to use...
I0226 20:14:23.313439   17709 detect.go:190] detected "systemd" cgroup driver on host os
I0226 20:14:23.313529   17709 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0226 20:14:23.333174   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0226 20:14:23.345404   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0226 20:14:23.357405   17709 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0226 20:14:23.357452   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0226 20:14:23.369026   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0226 20:14:23.380664   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0226 20:14:23.392729   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0226 20:14:23.403839   17709 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0226 20:14:23.414883   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0226 20:14:23.426652   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0226 20:14:23.439046   17709 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0226 20:14:23.450500   17709 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0226 20:14:23.462063   17709 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0226 20:14:23.462110   17709 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0226 20:14:23.476649   17709 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0226 20:14:23.488235   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:23.593744   17709 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0226 20:14:23.699811   17709 start.go:495] detecting cgroup driver to use...
I0226 20:14:23.699842   17709 detect.go:190] detected "systemd" cgroup driver on host os
I0226 20:14:23.699924   17709 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0226 20:14:23.715575   17709 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0226 20:14:23.715637   17709 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0226 20:14:23.735170   17709 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0226 20:14:23.758018   17709 ssh_runner.go:195] Run: which cri-dockerd
I0226 20:14:23.763584   17709 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0226 20:14:23.775018   17709 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0226 20:14:23.800929   17709 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0226 20:14:23.953210   17709 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0226 20:14:24.067850   17709 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0226 20:14:24.067955   17709 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0226 20:14:24.090878   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:24.189776   17709 ssh_runner.go:195] Run: sudo systemctl restart docker
I0226 20:14:25.574304   17709 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.384503657s)
I0226 20:14:25.574363   17709 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0226 20:14:25.588770   17709 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0226 20:14:25.604141   17709 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0226 20:14:25.618908   17709 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0226 20:14:25.716240   17709 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0226 20:14:25.802290   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:25.899909   17709 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0226 20:14:25.927488   17709 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0226 20:14:25.940128   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:26.042818   17709 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0226 20:14:26.588776   17709 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0226 20:14:26.588958   17709 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0226 20:14:26.597366   17709 start.go:563] Will wait 60s for crictl version
I0226 20:14:26.597457   17709 ssh_runner.go:195] Run: which crictl
I0226 20:14:26.603014   17709 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0226 20:14:26.859882   17709 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0226 20:14:26.859936   17709 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0226 20:14:27.082738   17709 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0226 20:14:27.119081   17709 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0226 20:14:27.119175   17709 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0226 20:14:27.140490   17709 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0226 20:14:27.147583   17709 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0226 20:14:27.162161   17709 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/trungnt:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0226 20:14:27.162274   17709 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0226 20:14:27.162336   17709 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0226 20:14:27.182840   17709 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0226 20:14:27.182849   17709 docker.go:619] Images already preloaded, skipping extraction
I0226 20:14:27.182902   17709 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0226 20:14:27.202713   17709 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0226 20:14:27.202723   17709 cache_images.go:84] Images are preloaded, skipping loading
I0226 20:14:27.202730   17709 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0226 20:14:27.202831   17709 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0226 20:14:27.202879   17709 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0226 20:14:27.600863   17709 cni.go:84] Creating CNI manager for ""
I0226 20:14:27.600929   17709 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0226 20:14:27.601101   17709 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0226 20:14:27.601155   17709 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0226 20:14:27.601480   17709 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0226 20:14:27.601670   17709 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0226 20:14:27.634600   17709 binaries.go:44] Found k8s binaries, skipping transfer
I0226 20:14:27.634766   17709 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0226 20:14:27.651449   17709 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0226 20:14:27.679605   17709 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0226 20:14:27.703575   17709 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0226 20:14:27.725707   17709 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0226 20:14:27.730120   17709 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0226 20:14:27.746202   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:27.832213   17709 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0226 20:14:27.854696   17709 certs.go:68] Setting up /home/trungnt/.minikube/profiles/minikube for IP: 192.168.49.2
I0226 20:14:27.854715   17709 certs.go:194] generating shared ca certs ...
I0226 20:14:27.854730   17709 certs.go:226] acquiring lock for ca certs: {Name:mka81fd020cccb0d214c97a307daf398b7756ef2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 20:14:27.854925   17709 certs.go:235] skipping valid "minikubeCA" ca cert: /home/trungnt/.minikube/ca.key
I0226 20:14:27.855187   17709 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/trungnt/.minikube/proxy-client-ca.key
I0226 20:14:27.855195   17709 certs.go:256] generating profile certs ...
I0226 20:14:27.855289   17709 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/trungnt/.minikube/profiles/minikube/client.key
I0226 20:14:27.856131   17709 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/trungnt/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0226 20:14:27.856377   17709 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/trungnt/.minikube/profiles/minikube/proxy-client.key
I0226 20:14:27.856620   17709 certs.go:484] found cert: /home/trungnt/.minikube/certs/ca-key.pem (1679 bytes)
I0226 20:14:27.856665   17709 certs.go:484] found cert: /home/trungnt/.minikube/certs/ca.pem (1078 bytes)
I0226 20:14:27.856719   17709 certs.go:484] found cert: /home/trungnt/.minikube/certs/cert.pem (1123 bytes)
I0226 20:14:27.856756   17709 certs.go:484] found cert: /home/trungnt/.minikube/certs/key.pem (1679 bytes)
I0226 20:14:27.857572   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0226 20:14:27.893605   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0226 20:14:27.934028   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0226 20:14:27.972713   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0226 20:14:28.008395   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0226 20:14:28.044042   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0226 20:14:28.078282   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0226 20:14:28.135277   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0226 20:14:28.188332   17709 ssh_runner.go:362] scp /home/trungnt/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0226 20:14:28.230643   17709 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0226 20:14:28.259051   17709 ssh_runner.go:195] Run: openssl version
I0226 20:14:28.271961   17709 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0226 20:14:28.285174   17709 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0226 20:14:28.289562   17709 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 25 03:25 /usr/share/ca-certificates/minikubeCA.pem
I0226 20:14:28.289615   17709 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0226 20:14:28.298468   17709 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0226 20:14:28.313954   17709 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0226 20:14:28.319009   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0226 20:14:28.327734   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0226 20:14:28.339137   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0226 20:14:28.353821   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0226 20:14:28.368613   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0226 20:14:28.383034   17709 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0226 20:14:28.397028   17709 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/trungnt:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 20:14:28.397198   17709 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0226 20:14:28.430970   17709 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0226 20:14:28.446219   17709 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0226 20:14:28.446228   17709 kubeadm.go:593] restartPrimaryControlPlane start ...
I0226 20:14:28.446277   17709 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0226 20:14:28.459771   17709 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0226 20:14:28.461047   17709 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0226 20:14:28.481536   17709 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0226 20:14:28.494217   17709 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0226 20:14:28.494237   17709 kubeadm.go:597] duration metric: took 48.004546ms to restartPrimaryControlPlane
I0226 20:14:28.494245   17709 kubeadm.go:394] duration metric: took 97.225366ms to StartCluster
I0226 20:14:28.494259   17709 settings.go:142] acquiring lock: {Name:mk72f4be12edc582c0fddae3b956b4a5d1cf7078 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 20:14:28.494361   17709 settings.go:150] Updating kubeconfig:  /home/trungnt/.kube/config
I0226 20:14:28.495959   17709 lock.go:35] WriteFile acquiring /home/trungnt/.kube/config: {Name:mkda0ace63020e1b83a81fd523554c9d9a566236 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 20:14:28.496196   17709 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0226 20:14:28.496259   17709 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0226 20:14:28.496356   17709 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0226 20:14:28.496381   17709 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0226 20:14:28.496388   17709 addons.go:247] addon storage-provisioner should already be in state true
I0226 20:14:28.496386   17709 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 20:14:28.496411   17709 host.go:66] Checking if "minikube" exists ...
I0226 20:14:28.496410   17709 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0226 20:14:28.496424   17709 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0226 20:14:28.496792   17709 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 20:14:28.496881   17709 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 20:14:28.503421   17709 out.go:177] üîé  Verifying Kubernetes components...
I0226 20:14:28.504720   17709 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 20:14:28.529774   17709 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0226 20:14:28.530809   17709 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0226 20:14:28.530822   17709 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0226 20:14:28.530894   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:28.537488   17709 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0226 20:14:28.537505   17709 addons.go:247] addon default-storageclass should already be in state true
I0226 20:14:28.537539   17709 host.go:66] Checking if "minikube" exists ...
I0226 20:14:28.538244   17709 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 20:14:28.593797   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:28.597372   17709 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0226 20:14:28.597385   17709 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0226 20:14:28.597486   17709 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 20:14:28.631342   17709 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/trungnt/.minikube/machines/minikube/id_rsa Username:docker}
I0226 20:14:28.694203   17709 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0226 20:14:28.724143   17709 api_server.go:52] waiting for apiserver process to appear ...
I0226 20:14:28.724693   17709 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0226 20:14:28.750562   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0226 20:14:28.772889   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0226 20:14:29.224421   17709 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0226 20:14:29.268303   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.268333   17709 retry.go:31] will retry after 345.244712ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0226 20:14:29.268592   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.268618   17709 retry.go:31] will retry after 284.199688ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.553836   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0226 20:14:29.614654   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0226 20:14:29.718842   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.718859   17709 retry.go:31] will retry after 262.833456ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.724996   17709 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0226 20:14:29.731903   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.731921   17709 retry.go:31] will retry after 388.992698ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:29.747238   17709 api_server.go:72] duration metric: took 1.251021404s to wait for apiserver process to appear ...
I0226 20:14:29.747251   17709 api_server.go:88] waiting for apiserver healthz status ...
I0226 20:14:29.747264   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:29.747558   17709 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0226 20:14:29.981905   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0226 20:14:30.067591   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:30.067605   17709 retry.go:31] will retry after 567.976004ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:30.122002   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0226 20:14:30.248201   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:30.248617   17709 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W0226 20:14:30.261643   17709 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:30.261661   17709 retry.go:31] will retry after 733.110929ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 20:14:30.637796   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0226 20:14:30.753762   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:30.995288   17709 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0226 20:14:33.055256   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0226 20:14:33.055285   17709 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0226 20:14:33.055297   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:33.122770   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0226 20:14:33.122787   17709 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0226 20:14:33.248485   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:33.255128   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0226 20:14:33.255149   17709 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0226 20:14:33.748201   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:33.753952   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0226 20:14:33.753978   17709 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0226 20:14:33.816988   17709 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.179166643s)
I0226 20:14:33.817049   17709 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.821744219s)
I0226 20:14:33.859228   17709 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0226 20:14:33.860296   17709 addons.go:514] duration metric: took 5.364049649s for enable addons: enabled=[storage-provisioner default-storageclass]
I0226 20:14:34.247904   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:34.256637   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0226 20:14:34.256654   17709 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0226 20:14:34.747816   17709 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0226 20:14:34.752902   17709 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0226 20:14:34.753902   17709 api_server.go:141] control plane version: v1.32.0
I0226 20:14:34.753917   17709 api_server.go:131] duration metric: took 5.006660735s to wait for apiserver health ...
I0226 20:14:34.753926   17709 system_pods.go:43] waiting for kube-system pods to appear ...
I0226 20:14:34.766656   17709 system_pods.go:59] 7 kube-system pods found
I0226 20:14:34.766696   17709 system_pods.go:61] "coredns-668d6bf9bc-4btp2" [656d1614-0a70-404b-ad54-7cae2a35fc6a] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0226 20:14:34.766706   17709 system_pods.go:61] "etcd-minikube" [95088907-c1aa-4671-933a-bc266cb43df1] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0226 20:14:34.766714   17709 system_pods.go:61] "kube-apiserver-minikube" [546f1546-cf42-419f-9d9f-f7c93669b1e5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0226 20:14:34.766722   17709 system_pods.go:61] "kube-controller-manager-minikube" [8966021b-6144-4a5a-8f9f-2ed52155e9d5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0226 20:14:34.766733   17709 system_pods.go:61] "kube-proxy-c5wmr" [eaceed65-488f-484b-8339-987f84807036] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0226 20:14:34.766741   17709 system_pods.go:61] "kube-scheduler-minikube" [65284280-21fa-4a58-aa14-41a604f077ba] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0226 20:14:34.766746   17709 system_pods.go:61] "storage-provisioner" [e00b4934-3d99-4d34-bba3-66e2fa2e637b] Running
I0226 20:14:34.766753   17709 system_pods.go:74] duration metric: took 12.820915ms to wait for pod list to return data ...
I0226 20:14:34.766763   17709 kubeadm.go:582] duration metric: took 6.270547389s to wait for: map[apiserver:true system_pods:true]
I0226 20:14:34.766775   17709 node_conditions.go:102] verifying NodePressure condition ...
I0226 20:14:34.772373   17709 node_conditions.go:122] node storage ephemeral capacity is 243937628Ki
I0226 20:14:34.772395   17709 node_conditions.go:123] node cpu capacity is 4
I0226 20:14:34.772406   17709 node_conditions.go:105] duration metric: took 5.626754ms to run NodePressure ...
I0226 20:14:34.772418   17709 start.go:241] waiting for startup goroutines ...
I0226 20:14:34.772428   17709 start.go:246] waiting for cluster config update ...
I0226 20:14:34.772441   17709 start.go:255] writing updated cluster config ...
I0226 20:14:34.772752   17709 ssh_runner.go:195] Run: rm -f paused
I0226 20:14:34.851559   17709 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0226 20:14:34.856498   17709 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 26 13:14:26 minikube cri-dockerd[1237]: time="2025-02-26T13:14:26Z" level=info msg="Setting cgroupDriver systemd"
Feb 26 13:14:26 minikube cri-dockerd[1237]: time="2025-02-26T13:14:26Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 26 13:14:26 minikube cri-dockerd[1237]: time="2025-02-26T13:14:26Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 26 13:14:26 minikube cri-dockerd[1237]: time="2025-02-26T13:14:26Z" level=info msg="Start cri-dockerd grpc backend"
Feb 26 13:14:26 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 26 13:14:28 minikube cri-dockerd[1237]: time="2025-02-26T13:14:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-4btp2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d1eacf1589413d39ba3ea91a20a8e914d0d61f8801af5d6cddaa84df2c38aa07\""
Feb 26 13:14:29 minikube cri-dockerd[1237]: time="2025-02-26T13:14:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c01bbc8b9c1061ec9f901b13ee5feb71808116f5587a1e1c74ba53b61a44d8b5/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:29 minikube cri-dockerd[1237]: time="2025-02-26T13:14:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a889ee24f7c9ae856b5eb8c49e24bbfe9f84761defa2b411aa81d648cdeb9ada/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:29 minikube cri-dockerd[1237]: time="2025-02-26T13:14:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f4bceea464aee42ba0397d3ffc739bcc1688aeed26d8673701e0ab9f1cc718fd/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:29 minikube cri-dockerd[1237]: time="2025-02-26T13:14:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/292698fb16f5240fe0648c65d46b93cc8faa4e0d4d7b8369d3fa460930ac3cf7/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:33 minikube cri-dockerd[1237]: time="2025-02-26T13:14:33Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 26 13:14:34 minikube cri-dockerd[1237]: time="2025-02-26T13:14:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/579772ddd354890ee1746a27bf4284e0234568c551025245d3c237a775dc292c/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:34 minikube cri-dockerd[1237]: time="2025-02-26T13:14:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d76c52f92cbf1dedb078bde8228b91cc5a803256a5124cd70c899771dadd2f69/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:14:34 minikube cri-dockerd[1237]: time="2025-02-26T13:14:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d804f63e8bed99a20feb9e762ee97f5f4d9913d6e143d7c62e971f63a30e2f0/resolv.conf as [nameserver 125.235.4.59 options edns0 trust-ad ndots:0]"
Feb 26 13:15:04 minikube dockerd[952]: time="2025-02-26T13:15:04.517762213Z" level=info msg="ignoring event" container=558d6fcf12f1461f95b1f0c856af18320e4d2482527c9b042ddf29a4119264f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:17:25 minikube cri-dockerd[1237]: time="2025-02-26T13:17:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22effb521baefb980769533eab20a79d13c2dccef75d7eb792819c2972f812bd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:17:25 minikube cri-dockerd[1237]: time="2025-02-26T13:17:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fc6ee853edd3a87a1e45a3be4841c239f3732ec41bb8586e59242723db06c085/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:17:38 minikube cri-dockerd[1237]: time="2025-02-26T13:17:38Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-backend:latest: Status: Downloaded newer image for scofieldtt/my-fullstack-backend:latest"
Feb 26 13:17:41 minikube cri-dockerd[1237]: time="2025-02-26T13:17:41Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-backend:latest: Status: Image is up to date for scofieldtt/my-fullstack-backend:latest"
Feb 26 13:19:33 minikube cri-dockerd[1237]: time="2025-02-26T13:19:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/17cc965230e2e200eb22d10d6c44a6ca873fbb06bfa85a4b69aa8e77f2d4e513/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:19:33 minikube cri-dockerd[1237]: time="2025-02-26T13:19:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/99ab1f53e44e10f3d37a6632fdc64cd7b1c927bebee6ffb63924ae916ca4a4b3/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:19:33 minikube dockerd[952]: time="2025-02-26T13:19:33.575835958Z" level=warning msg="reference for unknown type: " digest="sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 26 13:19:44 minikube cri-dockerd[1237]: time="2025-02-26T13:19:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=======>                                           ]  3.669MB/25.36MB"
Feb 26 13:19:54 minikube cri-dockerd[1237]: time="2025-02-26T13:19:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=====================================>             ]  18.87MB/25.36MB"
Feb 26 13:20:00 minikube cri-dockerd[1237]: time="2025-02-26T13:20:00Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 26 13:20:00 minikube cri-dockerd[1237]: time="2025-02-26T13:20:00Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 26 13:20:00 minikube dockerd[952]: time="2025-02-26T13:20:00.538098644Z" level=info msg="ignoring event" container=efef0dfb451d5895f8203318ec9ed607e05f56612038405d9f6f4c2f73d44618 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:20:00 minikube dockerd[952]: time="2025-02-26T13:20:00.595902950Z" level=info msg="ignoring event" container=c6ea69e531ac8248e1e74f6d25e57c5fe6bdf808a6753d2be680013193979825 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:20:01 minikube dockerd[952]: time="2025-02-26T13:20:01.239571230Z" level=info msg="ignoring event" container=ce35eb4829216cace19d952549181df0a18ce6398a1f5b8eea6a180ca8d7e150 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:20:02 minikube dockerd[952]: time="2025-02-26T13:20:02.306584921Z" level=info msg="ignoring event" container=17cc965230e2e200eb22d10d6c44a6ca873fbb06bfa85a4b69aa8e77f2d4e513 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:20:03 minikube dockerd[952]: time="2025-02-26T13:20:03.312444292Z" level=info msg="ignoring event" container=99ab1f53e44e10f3d37a6632fdc64cd7b1c927bebee6ffb63924ae916ca4a4b3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:20:05 minikube cri-dockerd[1237]: time="2025-02-26T13:20:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d8bb0f6103373f4694eab105c44b487b206c0c66d2fadd908ba8af662dd133d/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:20:05 minikube dockerd[952]: time="2025-02-26T13:20:05.325315830Z" level=warning msg="reference for unknown type: " digest="sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7" remote="registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Feb 26 13:20:16 minikube cri-dockerd[1237]: time="2025-02-26T13:20:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [==================================>                ]  13.76MB/19.87MB"
Feb 26 13:20:24 minikube cri-dockerd[1237]: time="2025-02-26T13:20:24Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Feb 26 13:38:04 minikube cri-dockerd[1237]: time="2025-02-26T13:38:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/26bb47275d40ccbae13c099fc3329952708159e84601554354a847b36506965d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:38:04 minikube cri-dockerd[1237]: time="2025-02-26T13:38:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6e2bc9a09aa55cd1b02c335cf9aca2aa1db10616f6ca1316d01c9151c7cfd691/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:38:09 minikube cri-dockerd[1237]: time="2025-02-26T13:38:09Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-backend:latest: Status: Image is up to date for scofieldtt/my-fullstack-backend:latest"
Feb 26 13:38:13 minikube cri-dockerd[1237]: time="2025-02-26T13:38:13Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-backend:latest: Status: Image is up to date for scofieldtt/my-fullstack-backend:latest"
Feb 26 13:38:42 minikube cri-dockerd[1237]: time="2025-02-26T13:38:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/24fa496034f3227f49539baae948a92bb2553e281cc7d21012b8e2061abe120c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:38:55 minikube cri-dockerd[1237]: time="2025-02-26T13:38:55Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-frontend:latest: Status: Downloaded newer image for scofieldtt/my-fullstack-frontend:latest"
Feb 26 13:38:57 minikube cri-dockerd[1237]: time="2025-02-26T13:38:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/005734f5a6c9716deb5aeeb91f63ee6aa04687fa9b1415f52f80ea14af00cffc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:39:02 minikube cri-dockerd[1237]: time="2025-02-26T13:39:02Z" level=info msg="Stop pulling image scofieldtt/my-fullstack-frontend:latest: Status: Image is up to date for scofieldtt/my-fullstack-frontend:latest"
Feb 26 13:39:26 minikube dockerd[952]: time="2025-02-26T13:39:26.926819527Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751
Feb 26 13:39:27 minikube dockerd[952]: time="2025-02-26T13:39:27.009890986Z" level=info msg="ignoring event" container=e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:39:27 minikube dockerd[952]: time="2025-02-26T13:39:27.270850639Z" level=info msg="ignoring event" container=6e2bc9a09aa55cd1b02c335cf9aca2aa1db10616f6ca1316d01c9151c7cfd691 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:39:33 minikube dockerd[952]: time="2025-02-26T13:39:33.124591064Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22
Feb 26 13:39:33 minikube dockerd[952]: time="2025-02-26T13:39:33.168410492Z" level=info msg="ignoring event" container=88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:39:33 minikube dockerd[952]: time="2025-02-26T13:39:33.334870258Z" level=info msg="ignoring event" container=26bb47275d40ccbae13c099fc3329952708159e84601554354a847b36506965d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:10 minikube dockerd[952]: time="2025-02-26T13:51:10.317883573Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=0d2acd0d28fa836d1f6042f41ae3537cc8314d3e6a1b596e9e3bfc68c2faa458
Feb 26 13:51:10 minikube dockerd[952]: time="2025-02-26T13:51:10.425408555Z" level=info msg="ignoring event" container=0d2acd0d28fa836d1f6042f41ae3537cc8314d3e6a1b596e9e3bfc68c2faa458 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:10 minikube dockerd[952]: time="2025-02-26T13:51:10.680484219Z" level=info msg="ignoring event" container=5d8bb0f6103373f4694eab105c44b487b206c0c66d2fadd908ba8af662dd133d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:19 minikube cri-dockerd[1237]: time="2025-02-26T13:51:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3d6a698a638e595109aa4b6380d093b3a657cc2760bb45557d1ac4b356339a81/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:51:19 minikube cri-dockerd[1237]: time="2025-02-26T13:51:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1f06a688ceb2cb8fa829e2533aec1df9044a7972e3bcd36129b4280e883d282/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:51:20 minikube dockerd[952]: time="2025-02-26T13:51:20.368157282Z" level=info msg="ignoring event" container=b7caa1cd2c91857306af307efc0094d7921eedc32b2e1261b6780d6cb79270df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:20 minikube dockerd[952]: time="2025-02-26T13:51:20.383222966Z" level=info msg="ignoring event" container=8da69f71c749a7bdb1d850bea1ae03e8fc0d383f7479f99c434cc5115a7a5165 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:21 minikube cri-dockerd[1237]: time="2025-02-26T13:51:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0f63372ec92f39489545153c8e442ceb3b2aff8a5a472eb4381a36a8857a3a45/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 26 13:51:21 minikube dockerd[952]: time="2025-02-26T13:51:21.459678441Z" level=info msg="ignoring event" container=68afacb2cb93960622c462732c421571a1fda695c8e67fe2b57bc07b0a828a4b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:22 minikube dockerd[952]: time="2025-02-26T13:51:22.541319656Z" level=info msg="ignoring event" container=3d6a698a638e595109aa4b6380d093b3a657cc2760bb45557d1ac4b356339a81 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 26 13:51:23 minikube dockerd[952]: time="2025-02-26T13:51:23.605340746Z" level=info msg="ignoring event" container=a1f06a688ceb2cb8fa829e2533aec1df9044a7972e3bcd36129b4280e883d282 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e1033ba637a3a       ee44bc2368033                                                                                              48 seconds ago      Running             controller                0                   0f63372ec92f3       ingress-nginx-controller-56d7c84fd4-pl99z
68afacb2cb939       a62eeff05ba51                                                                                              48 seconds ago      Exited              patch                     1                   a1f06a688ceb2       ingress-nginx-admission-patch-5kqt5
8da69f71c749a       a62eeff05ba51                                                                                              49 seconds ago      Exited              create                    0                   3d6a698a638e5       ingress-nginx-admission-create-dncvq
3261f9dea0d49       scofieldtt/my-fullstack-frontend@sha256:dd0c51f7251404d03dc63a7d30b383fc4bf262834227d31da36fefcd1a1ae70e   13 minutes ago      Running             frontend                  0                   005734f5a6c97       frontend-5578567987-mkrff
22ab2cbed1c6e       scofieldtt/my-fullstack-frontend@sha256:dd0c51f7251404d03dc63a7d30b383fc4bf262834227d31da36fefcd1a1ae70e   13 minutes ago      Running             frontend                  0                   24fa496034f32       frontend-5578567987-vqxns
9857e87f29d85       scofieldtt/my-fullstack-backend@sha256:c875cd18c4918571186a98496af55ed5c70ca3a762f03540fca0834d837ff5a0    34 minutes ago      Running             backend                   0                   fc6ee853edd3a       backend-764bd84576-mtc6c
f5fa4c68fc2dd       scofieldtt/my-fullstack-backend@sha256:c875cd18c4918571186a98496af55ed5c70ca3a762f03540fca0834d837ff5a0    34 minutes ago      Running             backend                   0                   22effb521baef       backend-764bd84576-cjkjv
d722a02367e88       6e38f40d628db                                                                                              36 minutes ago      Running             storage-provisioner       3                   579772ddd3548       storage-provisioner
e11be31c65ba1       c69fa2e9cbf5f                                                                                              37 minutes ago      Running             coredns                   1                   4d804f63e8bed       coredns-668d6bf9bc-4btp2
e1b6408c137a5       040f9f8aac8cd                                                                                              37 minutes ago      Running             kube-proxy                1                   d76c52f92cbf1       kube-proxy-c5wmr
558d6fcf12f14       6e38f40d628db                                                                                              37 minutes ago      Exited              storage-provisioner       2                   579772ddd3548       storage-provisioner
6175fa12524b8       c2e17b8d0f4a3                                                                                              37 minutes ago      Running             kube-apiserver            1                   292698fb16f52       kube-apiserver-minikube
30f82c346953b       a389e107f4ff1                                                                                              37 minutes ago      Running             kube-scheduler            1                   f4bceea464aee       kube-scheduler-minikube
c57e0455de570       8cab3d2a8bd0f                                                                                              37 minutes ago      Running             kube-controller-manager   1                   a889ee24f7c9a       kube-controller-manager-minikube
ff25f1f7a40e7       a9e7e6b294baf                                                                                              37 minutes ago      Running             etcd                      1                   c01bbc8b9c106       etcd-minikube
43c17d5a4b9a5       c69fa2e9cbf5f                                                                                              34 hours ago        Exited              coredns                   0                   d1eacf1589413       coredns-668d6bf9bc-4btp2
54a867a4bc801       040f9f8aac8cd                                                                                              34 hours ago        Exited              kube-proxy                0                   9dd8313287c83       kube-proxy-c5wmr
1456ce97574af       a389e107f4ff1                                                                                              34 hours ago        Exited              kube-scheduler            0                   6cd848ee344db       kube-scheduler-minikube
b58b80a4701f4       8cab3d2a8bd0f                                                                                              34 hours ago        Exited              kube-controller-manager   0                   a3ff0e6195772       kube-controller-manager-minikube
013dbde152d6f       a9e7e6b294baf                                                                                              34 hours ago        Exited              etcd                      0                   7a84df2097731       etcd-minikube
a7b75ec0d638a       c2e17b8d0f4a3                                                                                              34 hours ago        Exited              kube-apiserver            0                   7a7f93c9edc54       kube-apiserver-minikube


==> controller_ingress [e1033ba637a3] <==
W0226 13:51:21.852001       7 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0226 13:51:21.852841       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0226 13:51:21.879902       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="32" git="v1.32.0" state="clean" commit="70d3cc986aa8221cd1dfb1121852688902d3bf53" platform="linux/amd64"
I0226 13:51:22.093300       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0226 13:51:22.126199       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0226 13:51:22.137625       7 nginx.go:271] "Starting NGINX Ingress controller"
I0226 13:51:22.143927       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"7d71fd7d-c4aa-40f9-ae2e-1f78e95f673d", APIVersion:"v1", ResourceVersion:"69069", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0226 13:51:22.149626       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"3f48c7bf-4e9f-4512-b3ae-21d1ae85ae62", APIVersion:"v1", ResourceVersion:"69070", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0226 13:51:22.149657       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"bc0206cb-7e0c-4e88-8561-3476ee6d1b52", APIVersion:"v1", ResourceVersion:"69071", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0226 13:51:23.243986       7 store.go:440] "Found valid IngressClass" ingress="default/fullstack-ingress" ingressclass="_"
I0226 13:51:23.244366       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"fullstack-ingress", UID:"b3a6c882-041a-476e-a4aa-509ce531aa5e", APIVersion:"networking.k8s.io/v1", ResourceVersion:"68465", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0226 13:51:23.340200       7 nginx.go:317] "Starting NGINX process"
I0226 13:51:23.340279       7 leaderelection.go:254] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0226 13:51:23.341577       7 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0226 13:51:23.343636       7 controller.go:193] "Configuration changes detected, backend reload required"
I0226 13:51:23.380878       7 leaderelection.go:268] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0226 13:51:23.387193       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-56d7c84fd4-pl99z"
I0226 13:51:23.445592       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-pl99z" node="minikube"
I0226 13:51:23.452492       7 status.go:304] "updating Ingress status" namespace="default" ingress="fullstack-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0226 13:51:23.462597       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"fullstack-ingress", UID:"b3a6c882-041a-476e-a4aa-509ce531aa5e", APIVersion:"networking.k8s.io/v1", ResourceVersion:"69145", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0226 13:51:23.511195       7 controller.go:213] "Backend successfully reloaded"
I0226 13:51:23.511271       7 controller.go:224] "Initial sync, sleeping for 1 second"
I0226 13:51:23.511412       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56d7c84fd4-pl99z", UID:"2645786c-50c6-4b36-96f0-38c212c05076", APIVersion:"v1", ResourceVersion:"69108", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.11.3
  Build:         0106de65cfccb74405a6dfa7d9daffc6f0a6ef1a
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------



==> coredns [43c17d5a4b9a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:48905 - 23765 "HINFO IN 8603931894217577796.9131913637716168994. udp 57 false 512" NOERROR qr,rd,ra 57 0.017570785s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1703476689]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 03:26:05.642) (total time: 30003ms):
Trace[1703476689]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (03:26:35.645)
Trace[1703476689]: [30.003752611s] [30.003752611s] END
[INFO] plugin/kubernetes: Trace[1963132238]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 03:26:05.642) (total time: 30003ms):
Trace[1963132238]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (03:26:35.645)
Trace[1963132238]: [30.003866418s] [30.003866418s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[143797790]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 03:26:05.642) (total time: 30003ms):
Trace[143797790]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (03:26:35.645)
Trace[143797790]: [30.003558882s] [30.003558882s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [e11be31c65ba] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:59923 - 713 "HINFO IN 5985955954012903317.8958927065927874321. udp 57 false 512" NOERROR qr,rd,ra 57 0.084938686s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1210445059]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 13:14:34.912) (total time: 30004ms):
Trace[1210445059]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (13:15:04.915)
Trace[1210445059]: [30.004198173s] [30.004198173s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1859201733]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 13:14:34.912) (total time: 30005ms):
Trace[1859201733]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (13:15:04.916)
Trace[1859201733]: [30.005941405s] [30.005941405s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1838284414]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 13:14:34.913) (total time: 30005ms):
Trace[1838284414]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (13:15:04.915)
Trace[1838284414]: [30.005690531s] [30.005690531s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.11:56944 - 41741 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00047865s
[INFO] 10.244.0.11:56944 - 42068 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000690931s
[INFO] 10.244.0.11:55445 - 55926 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000208587s
[INFO] 10.244.0.11:55445 - 55420 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000790405s
[INFO] 10.244.0.11:41966 - 29609 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000169674s
[INFO] 10.244.0.11:41966 - 29985 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00027928s
[INFO] 10.244.0.11:49501 - 32818 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.007640946s
[INFO] 10.244.0.11:49501 - 33151 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.043553205s
[INFO] 10.244.0.12:43706 - 3515 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000458517s
[INFO] 10.244.0.12:43706 - 4226 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.006767792s
[INFO] 10.244.0.12:44263 - 15190 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000336507s
[INFO] 10.244.0.12:44263 - 14499 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000249281s
[INFO] 10.244.0.12:41316 - 2015 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000279529s
[INFO] 10.244.0.12:41316 - 2687 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000316892s
[INFO] 10.244.0.12:57047 - 15887 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.000558819s
[INFO] 10.244.0.12:57047 - 15364 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.001440091s
[INFO] 10.244.0.11:56246 - 58144 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000225222s
[INFO] 10.244.0.11:56246 - 57793 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000244578s
[INFO] 10.244.0.11:59036 - 15547 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000157098s
[INFO] 10.244.0.11:59036 - 15235 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000146742s
[INFO] 10.244.0.11:35869 - 23749 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00015893s
[INFO] 10.244.0.11:35869 - 23418 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000431689s
[INFO] 10.244.0.11:56475 - 44666 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.000146265s
[INFO] 10.244.0.11:56475 - 44417 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000109889s
[INFO] 10.244.0.12:55469 - 59812 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000202737s
[INFO] 10.244.0.12:55469 - 59396 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000280841s
[INFO] 10.244.0.12:56054 - 38595 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000190803s
[INFO] 10.244.0.12:56054 - 38137 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000252878s
[INFO] 10.244.0.12:43516 - 56197 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000145441s
[INFO] 10.244.0.12:43516 - 56629 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000113053s
[INFO] 10.244.0.12:58430 - 40032 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000122772s
[INFO] 10.244.0.12:58430 - 40280 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.000244757s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_25T10_26_00_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 25 Feb 2025 03:25:56 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 26 Feb 2025 13:52:09 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 26 Feb 2025 13:48:43 +0000   Tue, 25 Feb 2025 03:25:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 26 Feb 2025 13:48:43 +0000   Tue, 25 Feb 2025 03:25:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 26 Feb 2025 13:48:43 +0000   Tue, 25 Feb 2025 03:25:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 26 Feb 2025 13:48:43 +0000   Tue, 25 Feb 2025 03:25:57 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  243937628Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12110712Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  243937628Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12110712Ki
  pods:               110
System Info:
  Machine ID:                 3ce81cfd8d364feeaf100d6cee951fe9
  System UUID:                db7746f3-a1cc-46c9-9b38-3b3a409574c9
  Boot ID:                    24f7d29a-0c24-4869-aab7-79b386260c1b
  Kernel Version:             6.11.0-17-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     backend-764bd84576-cjkjv                     250m (6%)     500m (12%)  256Mi (2%)       512Mi (4%)     34m
  default                     backend-764bd84576-mtc6c                     250m (6%)     500m (12%)  256Mi (2%)       512Mi (4%)     34m
  default                     frontend-5578567987-mkrff                    250m (6%)     500m (12%)  256Mi (2%)       512Mi (4%)     13m
  default                     frontend-5578567987-vqxns                    250m (6%)     500m (12%)  256Mi (2%)       512Mi (4%)     13m
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-pl99z    100m (2%)     0 (0%)      90Mi (0%)        0 (0%)         51s
  kube-system                 coredns-668d6bf9bc-4btp2                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     34h
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         34h
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         34h
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         34h
  kube-system                 kube-proxy-c5wmr                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         34h
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         34h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         34h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1850m (46%)   2 (50%)
  memory             1284Mi (10%)  2218Mi (18%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 37m                kube-proxy       
  Normal   Starting                 37m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  37m (x8 over 37m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    37m (x8 over 37m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     37m (x7 over 37m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  37m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 37m                kubelet          Node minikube has been rebooted, boot id: 24f7d29a-0c24-4869-aab7-79b386260c1b
  Normal   RegisteredNode           37m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Feb26 09:09] x86/cpu: SGX disabled by BIOS.
[  +0.051478] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.295598] i8042: Warning: Keylock active
[  +0.002928] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000099] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.027314] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.372904] wmi_bus wmi_bus-PNP0C14:00: [Firmware Bug]: WQBC data block query control method not found
[  +0.040017] r8169 0000:02:00.0: can't disable ASPM; OS doesn't have ASPM control
[  +0.503975] ata1.00: NCQ Send/Recv Log not supported
[  +0.002241] ata1.00: NCQ Send/Recv Log not supported
[  +2.521976] intel-hid INT33D5:00: failed to enable HID power button
[  +1.549662] kauditd_printk_skb: 126 callbacks suppressed
[  +0.618011] Bluetooth: hci0: Bad flag given (0x1) vs supported (0x0)
[  +8.066609] kauditd_printk_skb: 25 callbacks suppressed
[  +1.291798] psmouse serio1: Failed to deactivate mouse on isa0060/serio1: -5
[  +0.479986] psmouse serio1: Failed to enable mouse on isa0060/serio1
[  +4.552015] psmouse serio1: Failed to enable mouse on isa0060/serio1
[Feb26 09:10] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[Feb26 09:14] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Feb26 09:16] workqueue: delayed_fput hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[ +33.373420] workqueue: delayed_fput hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Feb26 09:18] workqueue: delayed_fput hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[Feb26 09:43] workqueue: delayed_fput hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[Feb26 10:19] workqueue: delayed_fput hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[Feb26 11:32] workqueue: delayed_fput hogged CPU for >10000us 67 times, consider switching to WQ_UNBOUND
[Feb26 11:34] ACPI Error: Thread 3524919296 cannot release Mutex [PATM] acquired by thread 1083867136 (20240322/exmutex-378)

[  +0.000018] No Local Variables are initialized for Method [_Q66]

[  +0.000003] No Arguments are initialized for method [_Q66]

[  +0.000005] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_AML_NOT_OWNER) (20240322/psparse-529)
[  +0.564052] psmouse serio1: Failed to disable mouse on isa0060/serio1
[  +0.636000] DMAR: DRHD: handling fault status reg 2
[  +0.000006] DMAR: [INTR-REMAP] Request device [f0:1f.0] fault index 0x0 [fault reason 0x25] Blocked a compatibility format interrupt request
[  +0.003337] iwlwifi 0000:01:00.0: RF_KILL bit toggled to enable radio.
[  +0.308602] ata1.00: NCQ Send/Recv Log not supported
[  +0.003160] ata1.00: NCQ Send/Recv Log not supported
[  +0.583458] done.
[  +0.133635] ACPI Error: Cannot release Mutex [PATM], not acquired (20240322/exmutex-357)

[  +0.000018] No Local Variables are initialized for Method [_Q66]

[  +0.000003] No Arguments are initialized for method [_Q66]

[  +0.000004] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_AML_MUTEX_NOT_ACQUIRED) (20240322/psparse-529)
[  +0.287962] Bluetooth: hci0: Bad flag given (0x1) vs supported (0x0)


==> etcd [013dbde152d6] <==
{"level":"info","ts":"2025-02-26T05:05:31.867520Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":62870,"took":"5.407504ms","hash":844973952,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":835584,"current-db-size-in-use":"836 kB"}
{"level":"info","ts":"2025-02-26T05:05:31.867669Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":844973952,"revision":62870,"compact-revision":62630}
{"level":"info","ts":"2025-02-26T05:10:31.868469Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":63110}
{"level":"info","ts":"2025-02-26T05:10:31.870982Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":63110,"took":"2.258059ms","hash":2431943559,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":831488,"current-db-size-in-use":"832 kB"}
{"level":"info","ts":"2025-02-26T05:10:31.871039Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2431943559,"revision":63110,"compact-revision":62870}
{"level":"info","ts":"2025-02-26T05:15:31.876163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":63348}
{"level":"info","ts":"2025-02-26T05:15:31.880486Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":63348,"took":"3.772578ms","hash":3326075446,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":835584,"current-db-size-in-use":"836 kB"}
{"level":"info","ts":"2025-02-26T05:15:31.880576Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3326075446,"revision":63348,"compact-revision":63110}
{"level":"info","ts":"2025-02-26T05:20:31.885452Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":63588}
{"level":"info","ts":"2025-02-26T05:20:31.891368Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":63588,"took":"5.37329ms","hash":3499783692,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T05:20:31.891490Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3499783692,"revision":63588,"compact-revision":63348}
{"level":"info","ts":"2025-02-26T05:20:39.146122Z","caller":"etcdserver/server.go:1473","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":80008,"local-member-snapshot-index":70007,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-02-26T05:20:39.151191Z","caller":"etcdserver/server.go:2493","msg":"saved snapshot","snapshot-index":80008}
{"level":"info","ts":"2025-02-26T05:20:39.151478Z","caller":"etcdserver/server.go:2523","msg":"compacted Raft logs","compact-index":75008}
{"level":"info","ts":"2025-02-26T05:21:00.581124Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000002-0000000000007533.snap"}
{"level":"info","ts":"2025-02-26T05:25:31.894838Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":63828}
{"level":"info","ts":"2025-02-26T05:25:31.899401Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":63828,"took":"3.940446ms","hash":4171900317,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T05:25:31.899499Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4171900317,"revision":63828,"compact-revision":63588}
{"level":"info","ts":"2025-02-26T05:30:31.904939Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":64067}
{"level":"info","ts":"2025-02-26T05:30:31.909300Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":64067,"took":"3.719698ms","hash":3107546583,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":831488,"current-db-size-in-use":"832 kB"}
{"level":"info","ts":"2025-02-26T05:30:31.909390Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3107546583,"revision":64067,"compact-revision":63828}
{"level":"warn","ts":"2025-02-26T05:31:12.863837Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"137.551369ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035508732878587 > lease_revoke:<id:70cc953b23c8deb0>","response":"size:30"}
{"level":"info","ts":"2025-02-26T05:35:31.913284Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":64306}
{"level":"info","ts":"2025-02-26T05:35:31.918974Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":64306,"took":"5.108903ms","hash":412144440,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":843776,"current-db-size-in-use":"844 kB"}
{"level":"info","ts":"2025-02-26T05:35:31.919144Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":412144440,"revision":64306,"compact-revision":64067}
{"level":"info","ts":"2025-02-26T05:40:31.921145Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":64546}
{"level":"info","ts":"2025-02-26T05:40:31.926521Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":64546,"took":"4.776972ms","hash":3503174011,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":835584,"current-db-size-in-use":"836 kB"}
{"level":"info","ts":"2025-02-26T05:40:31.926654Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3503174011,"revision":64546,"compact-revision":64306}
{"level":"info","ts":"2025-02-26T05:40:35.756972Z","caller":"traceutil/trace.go:171","msg":"trace[1327244004] transaction","detail":"{read_only:false; response_revision:64789; number_of_response:1; }","duration":"141.464417ms","start":"2025-02-26T05:40:35.615453Z","end":"2025-02-26T05:40:35.756918Z","steps":["trace[1327244004] 'process raft request'  (duration: 141.25142ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T05:45:31.934742Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":64786}
{"level":"info","ts":"2025-02-26T05:45:31.937386Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":64786,"took":"2.422038ms","hash":2923344127,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":835584,"current-db-size-in-use":"836 kB"}
{"level":"info","ts":"2025-02-26T05:45:31.937433Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2923344127,"revision":64786,"compact-revision":64546}
{"level":"info","ts":"2025-02-26T05:50:31.946541Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65025}
{"level":"info","ts":"2025-02-26T05:50:31.951199Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":65025,"took":"4.022878ms","hash":2553645882,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T05:50:31.951285Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2553645882,"revision":65025,"compact-revision":64786}
{"level":"info","ts":"2025-02-26T05:55:31.954292Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65264}
{"level":"info","ts":"2025-02-26T05:55:31.959239Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":65264,"took":"4.336662ms","hash":2032529563,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T05:55:31.959336Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2032529563,"revision":65264,"compact-revision":65025}
{"level":"info","ts":"2025-02-26T06:00:31.964606Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65504}
{"level":"info","ts":"2025-02-26T06:00:31.969426Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":65504,"took":"4.217004ms","hash":2320055755,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":843776,"current-db-size-in-use":"844 kB"}
{"level":"info","ts":"2025-02-26T06:00:31.969536Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2320055755,"revision":65504,"compact-revision":65264}
{"level":"info","ts":"2025-02-26T06:05:31.972624Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65744}
{"level":"info","ts":"2025-02-26T06:05:31.977689Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":65744,"took":"4.442037ms","hash":264965564,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":843776,"current-db-size-in-use":"844 kB"}
{"level":"info","ts":"2025-02-26T06:05:31.977777Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":264965564,"revision":65744,"compact-revision":65504}
{"level":"info","ts":"2025-02-26T06:10:31.977709Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65983}
{"level":"info","ts":"2025-02-26T06:10:31.979486Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":65983,"took":"1.582601ms","hash":4285496916,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T06:10:31.979538Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4285496916,"revision":65983,"compact-revision":65744}
{"level":"info","ts":"2025-02-26T06:15:31.989531Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":66222}
{"level":"info","ts":"2025-02-26T06:15:31.995721Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":66222,"took":"5.547358ms","hash":3751927196,"current-db-size-bytes":1376256,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":839680,"current-db-size-in-use":"840 kB"}
{"level":"info","ts":"2025-02-26T06:15:31.995835Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3751927196,"revision":66222,"compact-revision":65983}
{"level":"info","ts":"2025-02-26T06:19:15.832302Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-02-26T06:19:15.834755Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-02-26T06:19:15.839123Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-26T06:19:15.842244Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-26T06:19:15.884225Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-26T06:19:15.884268Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-02-26T06:19:15.892857Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-26T06:19:15.917868Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T06:19:15.921991Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T06:19:15.922021Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [ff25f1f7a40e] <==
{"level":"info","ts":"2025-02-26T13:14:30.833456Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":80008,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2025-02-26T13:14:30.835477Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":1376256,"backend-size":"1.4 MB","backend-size-in-use-bytes":753664,"backend-size-in-use":"754 kB"}
{"level":"info","ts":"2025-02-26T13:14:31.439788Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":83527}
{"level":"info","ts":"2025-02-26T13:14:31.440499Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-26T13:14:31.441198Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-02-26T13:14:31.441247Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 2, commit: 83527, applied: 80008, lastindex: 83527, lastterm: 2]"}
{"level":"info","ts":"2025-02-26T13:14:31.441662Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-26T13:14:31.442142Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-26T13:14:31.442161Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-02-26T13:14:31.442861Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-26T13:14:31.443822Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":66222}
{"level":"info","ts":"2025-02-26T13:14:31.449563Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":66639}
{"level":"info","ts":"2025-02-26T13:14:31.450658Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-26T13:14:31.452016Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-02-26T13:14:31.452931Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-26T13:14:31.453197Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-26T13:14:31.454531Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-26T13:14:31.457053Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T13:14:31.457420Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T13:14:31.457495Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T13:14:31.459677Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T13:14:31.468833Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-26T13:14:31.472349Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-26T13:14:31.473812Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-26T13:14:31.469149Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T13:14:31.478719Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T13:14:31.842665Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-02-26T13:14:31.846178Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-02-26T13:14:31.846365Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-26T13:14:31.847806Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-02-26T13:14:31.847948Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-02-26T13:14:31.848305Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-02-26T13:14:31.848729Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-02-26T13:14:31.851138Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-26T13:14:31.852056Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-26T13:14:31.854397Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-26T13:14:31.854500Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-26T13:14:31.855095Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-26T13:14:31.860331Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T13:14:31.861827Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-26T13:14:31.860390Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T13:14:31.874936Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-26T13:24:31.919041Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":66991}
{"level":"info","ts":"2025-02-26T13:24:31.938911Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":66991,"took":"18.909308ms","hash":1106973541,"current-db-size-bytes":2805760,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1662976,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T13:24:31.939025Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1106973541,"revision":66991,"compact-revision":66222}
{"level":"info","ts":"2025-02-26T13:29:31.931640Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67375}
{"level":"info","ts":"2025-02-26T13:29:31.943392Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":67375,"took":"10.889446ms","hash":3940919519,"current-db-size-bytes":2805760,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":2281472,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-02-26T13:29:31.943484Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3940919519,"revision":67375,"compact-revision":66991}
{"level":"info","ts":"2025-02-26T13:34:31.939409Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67654}
{"level":"info","ts":"2025-02-26T13:34:31.941712Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":67654,"took":"1.926056ms","hash":658936565,"current-db-size-bytes":2805760,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T13:34:31.941748Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":658936565,"revision":67654,"compact-revision":67375}
{"level":"info","ts":"2025-02-26T13:39:31.957505Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67934}
{"level":"info","ts":"2025-02-26T13:39:31.964426Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":67934,"took":"6.10043ms","hash":3438043489,"current-db-size-bytes":2805760,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":2088960,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-02-26T13:39:31.964566Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3438043489,"revision":67934,"compact-revision":67654}
{"level":"info","ts":"2025-02-26T13:44:31.972059Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68320}
{"level":"info","ts":"2025-02-26T13:44:31.982653Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":68320,"took":"10.230865ms","hash":2024049032,"current-db-size-bytes":2805760,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":2318336,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-02-26T13:44:31.982731Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2024049032,"revision":68320,"compact-revision":67934}
{"level":"info","ts":"2025-02-26T13:49:31.980372Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68610}
{"level":"info","ts":"2025-02-26T13:49:31.986549Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":68610,"took":"5.918577ms","hash":3480272387,"current-db-size-bytes":2936832,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":2019328,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-02-26T13:49:31.986584Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3480272387,"revision":68610,"compact-revision":68320}


==> kernel <==
 13:52:10 up  7:32,  0 users,  load average: 1.49, 2.22, 2.27
Linux minikube 6.11.0-17-generic #17~24.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 20 22:48:29 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [6175fa12524b] <==
I0226 13:14:33.040017       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0226 13:14:33.040024       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0226 13:14:33.040046       1 aggregator.go:169] waiting for initial CRD sync...
I0226 13:14:33.033018       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0226 13:14:33.040090       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0226 13:14:33.040613       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0226 13:14:33.041238       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0226 13:14:33.041794       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0226 13:14:33.041998       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0226 13:14:33.042012       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0226 13:14:33.042049       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0226 13:14:33.042062       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0226 13:14:33.032957       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0226 13:14:33.049531       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0226 13:14:33.049548       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0226 13:14:33.058919       1 controller.go:90] Starting OpenAPI V3 controller
I0226 13:14:33.059134       1 naming_controller.go:294] Starting NamingConditionController
I0226 13:14:33.059169       1 establishing_controller.go:81] Starting EstablishingController
I0226 13:14:33.059214       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0226 13:14:33.059228       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0226 13:14:33.059241       1 crd_finalizer.go:269] Starting CRDFinalizer
I0226 13:14:33.059470       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0226 13:14:33.059565       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0226 13:14:33.069601       1 controller.go:142] Starting OpenAPI controller
I0226 13:14:33.136414       1 shared_informer.go:320] Caches are synced for configmaps
I0226 13:14:33.139748       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0226 13:14:33.139779       1 policy_source.go:240] refreshing policies
I0226 13:14:33.149636       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0226 13:14:33.149679       1 aggregator.go:171] initial CRD sync complete...
I0226 13:14:33.149722       1 autoregister_controller.go:144] Starting autoregister controller
I0226 13:14:33.149735       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0226 13:14:33.166216       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0226 13:14:33.167343       1 cache.go:39] Caches are synced for RemoteAvailability controller
E0226 13:14:33.184088       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0226 13:14:33.206554       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0226 13:14:33.219353       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 07d9e8f7-00ee-4492-be12-6adca4e17727, UID in object meta: "
I0226 13:14:33.220677       1 shared_informer.go:320] Caches are synced for node_authorizer
I0226 13:14:33.242352       1 cache.go:39] Caches are synced for LocalAvailability controller
I0226 13:14:33.242378       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0226 13:14:33.243342       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0226 13:14:33.243361       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0226 13:14:33.250513       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0226 13:14:33.250763       1 cache.go:39] Caches are synced for autoregister controller
I0226 13:14:33.410242       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0226 13:14:33.656793       1 controller.go:615] quota admission added evaluator for: endpoints
I0226 13:14:34.041874       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0226 13:14:36.492337       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0226 13:14:36.756910       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0226 13:14:37.008843       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0226 13:17:34.825330       1 alloc.go:330] "allocated clusterIPs" service="default/backend-service" clusterIPs={"IPv4":"10.105.188.206"}
I0226 13:17:45.419976       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
I0226 13:19:32.464352       1 controller.go:615] quota admission added evaluator for: namespaces
I0226 13:19:32.491513       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0226 13:19:32.515497       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0226 13:19:32.571785       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.99.155.137"}
I0226 13:19:32.587065       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.97.234.215"}
I0226 13:19:32.609372       1 controller.go:615] quota admission added evaluator for: jobs.batch
I0226 13:34:37.224118       1 alloc.go:330] "allocated clusterIPs" service="default/frontend-service" clusterIPs={"IPv4":"10.109.217.134"}
I0226 13:51:19.174275       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.102.111.139"}
I0226 13:51:19.201051       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.105.75.190"}


==> kube-apiserver [a7b75ec0d638] <==
W0226 06:19:21.178671       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.187495       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.229016       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.235812       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.249967       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.255954       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.256800       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.264582       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.266286       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.286130       1 logging.go:55] [core] [Channel #22 SubChannel #2015]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.294902       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.298585       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.338102       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.355613       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.363569       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.470006       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.493162       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.551305       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.613697       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.635688       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.694242       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.753113       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:21.803302       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:23.877949       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:23.962908       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.160024       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.193338       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.242150       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.335449       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.474518       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.491853       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.518677       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.559187       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.599564       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.638545       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.721952       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.737290       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.746218       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.847153       1 logging.go:55] [core] [Channel #6824 SubChannel #6825]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.880593       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.929871       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.940946       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.944812       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:24.958289       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.012366       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.066844       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.091138       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.093743       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.116336       1 logging.go:55] [core] [Channel #25 SubChannel #2014]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.121216       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.195790       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.241834       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.242727       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.250516       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.257445       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.261171       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.286353       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.287390       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.290179       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0226 06:19:25.294145       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [b58b80a4701f] <==
I0226 01:10:35.504467       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:15:42.597549       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:20:47.278830       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:25:53.307438       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:30:58.047488       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:36:05.141455       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:41:11.753083       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:46:18.338877       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:51:26.039133       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 01:56:31.581159       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:01:38.367634       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:06:44.992419       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:11:51.288513       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:16:57.290391       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:22:03.599002       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:54:30.721535       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:58:39.304188       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0226 02:58:39.482730       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0226 02:59:32.515553       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="5.800659ms"
I0226 02:59:32.563486       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 02:59:36.869015       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:04:42.373165       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:09:48.593843       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:14:55.519570       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:20:01.958520       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:25:07.946207       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:30:13.971275       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:35:19.504406       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:40:26.483729       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:45:33.968930       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:50:41.473907       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 03:55:48.776726       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:00:54.321245       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:06:00.174357       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:11:07.161217       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:16:13.053701       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:21:18.334177       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:26:23.340630       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:31:29.577541       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:36:35.629523       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:41:42.059565       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:46:48.714249       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:51:54.536735       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 04:57:00.785797       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:02:06.249619       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:07:12.426272       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:12:18.616385       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:17:25.004159       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:22:30.814255       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:27:38.332818       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:32:45.234675       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:37:51.416158       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:42:56.203610       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:48:01.601075       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:53:09.081167       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 05:58:15.342747       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:03:21.563868       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:08:28.615496       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:13:34.939418       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:18:41.562293       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [c57e0455de57] <==
I0226 13:51:07.255197       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0226 13:51:07.272271       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
E0226 13:51:12.156344       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="daemonsets.apps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="apps/v1, Resource=daemonsets"
E0226 13:51:12.180051       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="networkpolicies.networking.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"networkpolicies\" in API group \"networking.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="networking.k8s.io/v1, Resource=networkpolicies"
E0226 13:51:12.190225       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="leases.coordination.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="coordination.k8s.io/v1, Resource=leases"
E0226 13:51:12.198043       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="podtemplates is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"podtemplates\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=podtemplates"
E0226 13:51:12.203420       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="persistentvolumeclaims is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"persistentvolumeclaims\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=persistentvolumeclaims"
E0226 13:51:12.207928       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="replicationcontrollers is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"replicationcontrollers\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=replicationcontrollers"
E0226 13:51:12.212497       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="jobs.batch is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"jobs\" in API group \"batch\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="batch/v1, Resource=jobs"
E0226 13:51:12.216894       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="rolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"rolebindings\" in API group \"rbac.authorization.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="rbac.authorization.k8s.io/v1, Resource=rolebindings"
E0226 13:51:12.221152       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="roles.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"roles\" in API group \"rbac.authorization.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="rbac.authorization.k8s.io/v1, Resource=roles"
E0226 13:51:12.225961       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="limitranges is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"limitranges\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=limitranges"
E0226 13:51:12.232068       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="serviceaccounts is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"serviceaccounts\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=serviceaccounts"
W0226 13:51:12.234314       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods is forbidden: User "system:serviceaccount:kube-system:namespace-controller" cannot watch resource "pods" in API group "" in the namespace "ingress-nginx"
E0226 13:51:12.238942       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="pods is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"pods\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=pods"
E0226 13:51:12.243649       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="deployments.apps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"deployments\" in API group \"apps\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="apps/v1, Resource=deployments"
E0226 13:51:12.248051       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="endpointslices.discovery.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"endpointslices\" in API group \"discovery.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="discovery.k8s.io/v1, Resource=endpointslices"
E0226 13:51:12.254251       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="configmaps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=configmaps"
E0226 13:51:12.325362       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="events is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"events\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=events"
E0226 13:51:12.330142       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ingresses.networking.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"ingresses\" in API group \"networking.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="networking.k8s.io/v1, Resource=ingresses"
E0226 13:51:12.334337       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="csistoragecapacities.storage.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"csistoragecapacities\" in API group \"storage.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="storage.k8s.io/v1, Resource=csistoragecapacities"
E0226 13:51:12.338825       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="controllerrevisions.apps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"controllerrevisions\" in API group \"apps\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="apps/v1, Resource=controllerrevisions"
E0226 13:51:12.343375       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="cronjobs.batch is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"cronjobs\" in API group \"batch\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="batch/v1, Resource=cronjobs"
E0226 13:51:12.347353       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="events.events.k8s.io is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"events\" in API group \"events.k8s.io\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="events.k8s.io/v1, Resource=events"
E0226 13:51:12.351663       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="services is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"services\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=services"
E0226 13:51:12.355878       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="horizontalpodautoscalers.autoscaling is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"horizontalpodautoscalers\" in API group \"autoscaling\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="autoscaling/v2, Resource=horizontalpodautoscalers"
E0226 13:51:12.360994       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="replicasets.apps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"replicasets\" in API group \"apps\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="apps/v1, Resource=replicasets"
E0226 13:51:12.365499       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="endpoints is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"endpoints\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=endpoints"
E0226 13:51:12.369643       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="resourcequotas is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"resourcequotas\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=resourcequotas"
E0226 13:51:12.374931       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="poddisruptionbudgets.policy is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"poddisruptionbudgets\" in API group \"policy\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="policy/v1, Resource=poddisruptionbudgets"
E0226 13:51:12.382035       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="secrets is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"secrets\" in API group \"\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="/v1, Resource=secrets"
E0226 13:51:12.386321       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="statefulsets.apps is forbidden: User \"system:serviceaccount:kube-system:namespace-controller\" cannot watch resource \"statefulsets\" in API group \"apps\" in the namespace \"ingress-nginx\"" logger="namespace-controller" resource="apps/v1, Resource=statefulsets"
I0226 13:51:17.391262       1 namespace_controller.go:187] "Namespace has been deleted" logger="namespace-controller" namespace="ingress-nginx"
I0226 13:51:19.233839       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0226 13:51:19.246659       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:19.269407       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:19.269497       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0226 13:51:19.272268       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:19.280634       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="52.550473ms"
I0226 13:51:19.305918       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:19.315306       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:19.321891       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="39.972577ms"
I0226 13:51:19.322044       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="88.851¬µs"
I0226 13:51:19.329196       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:19.337056       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:19.355609       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="73.767¬µs"
I0226 13:51:19.373584       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:21.234905       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:21.252661       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:22.376635       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="100.439¬µs"
I0226 13:51:22.420084       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:22.594434       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:23.491181       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:23.607515       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:23.620373       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0226 13:51:23.663706       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:24.520213       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:24.575487       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0226 13:51:33.584505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="47.0345ms"
I0226 13:51:33.585166       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="191.679¬µs"


==> kube-proxy [54a867a4bc80] <==
I0225 03:26:05.501921       1 server_linux.go:66] "Using iptables proxy"
I0225 03:26:05.661217       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0225 03:26:05.661292       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0225 03:26:05.693328       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0225 03:26:05.693380       1 server_linux.go:170] "Using iptables Proxier"
I0225 03:26:05.697624       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0225 03:26:05.704908       1 server.go:497] "Version info" version="v1.32.0"
I0225 03:26:05.705028       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 03:26:05.713407       1 config.go:105] "Starting endpoint slice config controller"
I0225 03:26:05.714354       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0225 03:26:05.714412       1 config.go:199] "Starting service config controller"
I0225 03:26:05.714425       1 shared_informer.go:313] Waiting for caches to sync for service config
I0225 03:26:05.716129       1 config.go:329] "Starting node config controller"
I0225 03:26:05.716148       1 shared_informer.go:313] Waiting for caches to sync for node config
I0225 03:26:05.815025       1 shared_informer.go:320] Caches are synced for service config
I0225 03:26:05.815025       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0225 03:26:05.817228       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [e1b6408c137a] <==
I0226 13:14:34.832000       1 server_linux.go:66] "Using iptables proxy"
I0226 13:14:35.065843       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0226 13:14:35.066101       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0226 13:14:35.187313       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0226 13:14:35.187372       1 server_linux.go:170] "Using iptables Proxier"
I0226 13:14:35.194528       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0226 13:14:35.203563       1 server.go:497] "Version info" version="v1.32.0"
I0226 13:14:35.204152       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0226 13:14:35.211906       1 config.go:199] "Starting service config controller"
I0226 13:14:35.213409       1 shared_informer.go:313] Waiting for caches to sync for service config
I0226 13:14:35.214092       1 config.go:105] "Starting endpoint slice config controller"
I0226 13:14:35.214136       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0226 13:14:35.214170       1 config.go:329] "Starting node config controller"
I0226 13:14:35.214178       1 shared_informer.go:313] Waiting for caches to sync for node config
I0226 13:14:35.313901       1 shared_informer.go:320] Caches are synced for service config
I0226 13:14:35.315004       1 shared_informer.go:320] Caches are synced for node config
I0226 13:14:35.315037       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [1456ce97574a] <==
I0225 03:25:56.580852       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0225 03:25:56.581464       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0225 03:25:56.581711       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0225 03:25:56.584717       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0225 03:25:56.584775       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0225 03:25:56.586740       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0225 03:25:56.586859       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.586999       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0225 03:25:56.587357       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0225 03:25:56.587367       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0225 03:25:56.587391       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0225 03:25:56.587396       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587035       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0225 03:25:56.587516       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587047       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0225 03:25:56.587551       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587048       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:56.587582       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587096       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:56.587619       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587108       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:56.587496       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0225 03:25:56.587653       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587185       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0225 03:25:56.587675       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587242       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0225 03:25:56.587698       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587265       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0225 03:25:56.587717       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587309       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0225 03:25:56.587738       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587315       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0225 03:25:56.587759       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:56.587762       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:56.587787       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.422687       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0225 03:25:57.422719       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.426344       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:57.426389       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.510444       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0225 03:25:57.510493       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.535802       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0225 03:25:57.536389       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.635756       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0225 03:25:57.635794       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.636703       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0225 03:25:57.636747       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.692260       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0225 03:25:57.692297       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0225 03:25:57.740575       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0225 03:25:57.740610       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.747959       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0225 03:25:57.747995       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0225 03:25:57.762277       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0225 03:25:57.762309       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0225 03:26:00.181977       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0226 06:19:15.885775       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0226 06:19:15.903480       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0226 06:19:15.903475       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0226 06:19:15.955413       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [30f82c346953] <==
I0226 13:14:31.094618       1 serving.go:386] Generated self-signed cert in-memory
W0226 13:14:33.112312       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0226 13:14:33.112350       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0226 13:14:33.112364       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0226 13:14:33.112387       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0226 13:14:33.190019       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0226 13:14:33.190050       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0226 13:14:33.195756       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0226 13:14:33.195800       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0226 13:14:33.197019       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0226 13:14:33.213245       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0226 13:14:33.299471       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 26 13:38:04 minikube kubelet[1437]: I0226 13:38:04.051650    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ptbs5\" (UniqueName: \"kubernetes.io/projected/910ceb19-890b-4262-b948-4bfaa89ff25c-kube-api-access-ptbs5\") pod \"frontend-b8b768b69-grhsm\" (UID: \"910ceb19-890b-4262-b948-4bfaa89ff25c\") " pod="default/frontend-b8b768b69-grhsm"
Feb 26 13:38:04 minikube kubelet[1437]: I0226 13:38:04.066107    1437 memory_manager.go:355] "RemoveStaleState removing state" podUID="4694d842-f19a-4d40-8040-235cf8c41cd6" containerName="patch"
Feb 26 13:38:04 minikube kubelet[1437]: I0226 13:38:04.151850    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w98jk\" (UniqueName: \"kubernetes.io/projected/8db0a497-604d-4c9e-b809-1a13587659b4-kube-api-access-w98jk\") pod \"frontend-b8b768b69-wrv8b\" (UID: \"8db0a497-604d-4c9e-b809-1a13587659b4\") " pod="default/frontend-b8b768b69-wrv8b"
Feb 26 13:38:10 minikube kubelet[1437]: I0226 13:38:10.271840    1437 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/frontend-b8b768b69-wrv8b" podStartSLOduration=1.677554282 podStartE2EDuration="6.271817242s" podCreationTimestamp="2025-02-26 13:38:04 +0000 UTC" firstStartedPulling="2025-02-26 13:38:04.877387632 +0000 UTC m=+1416.936469975" lastFinishedPulling="2025-02-26 13:38:09.471650528 +0000 UTC m=+1421.530732935" observedRunningTime="2025-02-26 13:38:10.271594483 +0000 UTC m=+1422.330676826" watchObservedRunningTime="2025-02-26 13:38:10.271817242 +0000 UTC m=+1422.330899600"
Feb 26 13:38:42 minikube kubelet[1437]: I0226 13:38:42.301439    1437 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/frontend-b8b768b69-grhsm" podStartSLOduration=30.124071684 podStartE2EDuration="38.301419631s" podCreationTimestamp="2025-02-26 13:38:04 +0000 UTC" firstStartedPulling="2025-02-26 13:38:04.887050441 +0000 UTC m=+1416.946132784" lastFinishedPulling="2025-02-26 13:38:13.064398304 +0000 UTC m=+1425.123480731" observedRunningTime="2025-02-26 13:38:14.446053601 +0000 UTC m=+1426.505135950" watchObservedRunningTime="2025-02-26 13:38:42.301419631 +0000 UTC m=+1454.360501983"
Feb 26 13:38:42 minikube kubelet[1437]: I0226 13:38:42.358011    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r2ntg\" (UniqueName: \"kubernetes.io/projected/72c4703e-f52c-4a75-beda-701a322b8579-kube-api-access-r2ntg\") pod \"frontend-5578567987-vqxns\" (UID: \"72c4703e-f52c-4a75-beda-701a322b8579\") " pod="default/frontend-5578567987-vqxns"
Feb 26 13:38:56 minikube kubelet[1437]: I0226 13:38:56.867579    1437 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/frontend-5578567987-vqxns" podStartSLOduration=2.675989209 podStartE2EDuration="14.867537913s" podCreationTimestamp="2025-02-26 13:38:42 +0000 UTC" firstStartedPulling="2025-02-26 13:38:43.033722487 +0000 UTC m=+1455.092804828" lastFinishedPulling="2025-02-26 13:38:55.22527112 +0000 UTC m=+1467.284353532" observedRunningTime="2025-02-26 13:38:56.866677891 +0000 UTC m=+1468.925760234" watchObservedRunningTime="2025-02-26 13:38:56.867537913 +0000 UTC m=+1468.926620256"
Feb 26 13:38:57 minikube kubelet[1437]: I0226 13:38:57.088558    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-59q8w\" (UniqueName: \"kubernetes.io/projected/d162510f-ccd7-426d-8e24-775e5f9935e9-kube-api-access-59q8w\") pod \"frontend-5578567987-mkrff\" (UID: \"d162510f-ccd7-426d-8e24-775e5f9935e9\") " pod="default/frontend-5578567987-mkrff"
Feb 26 13:39:03 minikube kubelet[1437]: I0226 13:39:03.105651    1437 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/frontend-5578567987-mkrff" podStartSLOduration=1.8202082480000001 podStartE2EDuration="7.10562615s" podCreationTimestamp="2025-02-26 13:38:56 +0000 UTC" firstStartedPulling="2025-02-26 13:38:57.547087091 +0000 UTC m=+1469.606169434" lastFinishedPulling="2025-02-26 13:39:02.832504907 +0000 UTC m=+1474.891587336" observedRunningTime="2025-02-26 13:39:03.076198526 +0000 UTC m=+1475.135280876" watchObservedRunningTime="2025-02-26 13:39:03.10562615 +0000 UTC m=+1475.164708495"
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.447811    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-w98jk\" (UniqueName: \"kubernetes.io/projected/8db0a497-604d-4c9e-b809-1a13587659b4-kube-api-access-w98jk\") pod \"8db0a497-604d-4c9e-b809-1a13587659b4\" (UID: \"8db0a497-604d-4c9e-b809-1a13587659b4\") "
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.456798    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8db0a497-604d-4c9e-b809-1a13587659b4-kube-api-access-w98jk" (OuterVolumeSpecName: "kube-api-access-w98jk") pod "8db0a497-604d-4c9e-b809-1a13587659b4" (UID: "8db0a497-604d-4c9e-b809-1a13587659b4"). InnerVolumeSpecName "kube-api-access-w98jk". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.549090    1437 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-w98jk\" (UniqueName: \"kubernetes.io/projected/8db0a497-604d-4c9e-b809-1a13587659b4-kube-api-access-w98jk\") on node \"minikube\" DevicePath \"\""
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.824654    1437 scope.go:117] "RemoveContainer" containerID="e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751"
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.850494    1437 scope.go:117] "RemoveContainer" containerID="e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751"
Feb 26 13:39:27 minikube kubelet[1437]: E0226 13:39:27.851441    1437 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751" containerID="e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751"
Feb 26 13:39:27 minikube kubelet[1437]: I0226 13:39:27.851491    1437 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751"} err="failed to get container status \"e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751\": rpc error: code = Unknown desc = Error response from daemon: No such container: e7e727d9b440ad631df9fceee3753cafc442ec8e71c9ca43e2850b40c5316751"
Feb 26 13:39:28 minikube kubelet[1437]: I0226 13:39:28.456866    1437 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8db0a497-604d-4c9e-b809-1a13587659b4" path="/var/lib/kubelet/pods/8db0a497-604d-4c9e-b809-1a13587659b4/volumes"
Feb 26 13:39:33 minikube kubelet[1437]: I0226 13:39:33.498655    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-ptbs5\" (UniqueName: \"kubernetes.io/projected/910ceb19-890b-4262-b948-4bfaa89ff25c-kube-api-access-ptbs5\") pod \"910ceb19-890b-4262-b948-4bfaa89ff25c\" (UID: \"910ceb19-890b-4262-b948-4bfaa89ff25c\") "
Feb 26 13:39:33 minikube kubelet[1437]: I0226 13:39:33.505823    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/910ceb19-890b-4262-b948-4bfaa89ff25c-kube-api-access-ptbs5" (OuterVolumeSpecName: "kube-api-access-ptbs5") pod "910ceb19-890b-4262-b948-4bfaa89ff25c" (UID: "910ceb19-890b-4262-b948-4bfaa89ff25c"). InnerVolumeSpecName "kube-api-access-ptbs5". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 26 13:39:33 minikube kubelet[1437]: I0226 13:39:33.599791    1437 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-ptbs5\" (UniqueName: \"kubernetes.io/projected/910ceb19-890b-4262-b948-4bfaa89ff25c-kube-api-access-ptbs5\") on node \"minikube\" DevicePath \"\""
Feb 26 13:39:34 minikube kubelet[1437]: I0226 13:39:34.023467    1437 scope.go:117] "RemoveContainer" containerID="88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22"
Feb 26 13:39:34 minikube kubelet[1437]: I0226 13:39:34.047326    1437 scope.go:117] "RemoveContainer" containerID="88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22"
Feb 26 13:39:34 minikube kubelet[1437]: E0226 13:39:34.048291    1437 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22" containerID="88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22"
Feb 26 13:39:34 minikube kubelet[1437]: I0226 13:39:34.048337    1437 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22"} err="failed to get container status \"88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22\": rpc error: code = Unknown desc = Error response from daemon: No such container: 88b4f4d8d3d2c6dd7968a73bfd17c39e5384440e12132c251e8efd3b9eec6e22"
Feb 26 13:39:34 minikube kubelet[1437]: I0226 13:39:34.407072    1437 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="910ceb19-890b-4262-b948-4bfaa89ff25c" path="/var/lib/kubelet/pods/910ceb19-890b-4262-b948-4bfaa89ff25c/volumes"
Feb 26 13:51:08 minikube kubelet[1437]: I0226 13:51:08.402518    1437 status_manager.go:890] "Failed to get status for pod" podUID="fa0ef472-c516-414f-9419-5ab458e8e0c0" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-ts9dt" err="pods \"ingress-nginx-controller-56d7c84fd4-ts9dt\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"ingress-nginx\": no relationship found between node 'minikube' and this object"
Feb 26 13:51:08 minikube kubelet[1437]: I0226 13:51:08.434268    1437 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="4694d842-f19a-4d40-8040-235cf8c41cd6" path="/var/lib/kubelet/pods/4694d842-f19a-4d40-8040-235cf8c41cd6/volumes"
Feb 26 13:51:08 minikube kubelet[1437]: I0226 13:51:08.434849    1437 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8ab17b70-bc3e-45c6-a231-117e82734691" path="/var/lib/kubelet/pods/8ab17b70-bc3e-45c6-a231-117e82734691/volumes"
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.843032    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hhhh7\" (UniqueName: \"kubernetes.io/projected/fa0ef472-c516-414f-9419-5ab458e8e0c0-kube-api-access-hhhh7\") pod \"fa0ef472-c516-414f-9419-5ab458e8e0c0\" (UID: \"fa0ef472-c516-414f-9419-5ab458e8e0c0\") "
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.843149    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/fa0ef472-c516-414f-9419-5ab458e8e0c0-webhook-cert\") pod \"fa0ef472-c516-414f-9419-5ab458e8e0c0\" (UID: \"fa0ef472-c516-414f-9419-5ab458e8e0c0\") "
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.851344    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fa0ef472-c516-414f-9419-5ab458e8e0c0-kube-api-access-hhhh7" (OuterVolumeSpecName: "kube-api-access-hhhh7") pod "fa0ef472-c516-414f-9419-5ab458e8e0c0" (UID: "fa0ef472-c516-414f-9419-5ab458e8e0c0"). InnerVolumeSpecName "kube-api-access-hhhh7". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.852625    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/fa0ef472-c516-414f-9419-5ab458e8e0c0-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "fa0ef472-c516-414f-9419-5ab458e8e0c0" (UID: "fa0ef472-c516-414f-9419-5ab458e8e0c0"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGIDValue ""
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.943767    1437 reconciler_common.go:299] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/fa0ef472-c516-414f-9419-5ab458e8e0c0-webhook-cert\") on node \"minikube\" DevicePath \"\""
Feb 26 13:51:10 minikube kubelet[1437]: I0226 13:51:10.943884    1437 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-hhhh7\" (UniqueName: \"kubernetes.io/projected/fa0ef472-c516-414f-9419-5ab458e8e0c0-kube-api-access-hhhh7\") on node \"minikube\" DevicePath \"\""
Feb 26 13:51:11 minikube kubelet[1437]: I0226 13:51:11.781541    1437 scope.go:117] "RemoveContainer" containerID="0d2acd0d28fa836d1f6042f41ae3537cc8314d3e6a1b596e9e3bfc68c2faa458"
Feb 26 13:51:12 minikube kubelet[1437]: I0226 13:51:12.410921    1437 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="fa0ef472-c516-414f-9419-5ab458e8e0c0" path="/var/lib/kubelet/pods/fa0ef472-c516-414f-9419-5ab458e8e0c0/volumes"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.274434    1437 memory_manager.go:355] "RemoveStaleState removing state" podUID="8db0a497-604d-4c9e-b809-1a13587659b4" containerName="frontend"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.274465    1437 memory_manager.go:355] "RemoveStaleState removing state" podUID="910ceb19-890b-4262-b948-4bfaa89ff25c" containerName="frontend"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.274476    1437 memory_manager.go:355] "RemoveStaleState removing state" podUID="fa0ef472-c516-414f-9419-5ab458e8e0c0" containerName="controller"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.311776    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lb9br\" (UniqueName: \"kubernetes.io/projected/4365767c-c243-459b-ac12-80e51f19a479-kube-api-access-lb9br\") pod \"ingress-nginx-admission-create-dncvq\" (UID: \"4365767c-c243-459b-ac12-80e51f19a479\") " pod="ingress-nginx/ingress-nginx-admission-create-dncvq"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.312091    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/2645786c-50c6-4b36-96f0-38c212c05076-webhook-cert\") pod \"ingress-nginx-controller-56d7c84fd4-pl99z\" (UID: \"2645786c-50c6-4b36-96f0-38c212c05076\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-pl99z"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.313782    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m2ndz\" (UniqueName: \"kubernetes.io/projected/2645786c-50c6-4b36-96f0-38c212c05076-kube-api-access-m2ndz\") pod \"ingress-nginx-controller-56d7c84fd4-pl99z\" (UID: \"2645786c-50c6-4b36-96f0-38c212c05076\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-pl99z"
Feb 26 13:51:19 minikube kubelet[1437]: I0226 13:51:19.414206    1437 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mfbdd\" (UniqueName: \"kubernetes.io/projected/8e3990c5-47a9-46a1-bdbe-8cd151c75b99-kube-api-access-mfbdd\") pod \"ingress-nginx-admission-patch-5kqt5\" (UID: \"8e3990c5-47a9-46a1-bdbe-8cd151c75b99\") " pod="ingress-nginx/ingress-nginx-admission-patch-5kqt5"
Feb 26 13:51:19 minikube kubelet[1437]: E0226 13:51:19.414356    1437 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 26 13:51:19 minikube kubelet[1437]: E0226 13:51:19.414432    1437 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/2645786c-50c6-4b36-96f0-38c212c05076-webhook-cert podName:2645786c-50c6-4b36-96f0-38c212c05076 nodeName:}" failed. No retries permitted until 2025-02-26 13:51:19.914408956 +0000 UTC m=+2211.973491301 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/2645786c-50c6-4b36-96f0-38c212c05076-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-pl99z" (UID: "2645786c-50c6-4b36-96f0-38c212c05076") : secret "ingress-nginx-admission" not found
Feb 26 13:51:19 minikube kubelet[1437]: E0226 13:51:19.918271    1437 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 26 13:51:19 minikube kubelet[1437]: E0226 13:51:19.918332    1437 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/2645786c-50c6-4b36-96f0-38c212c05076-webhook-cert podName:2645786c-50c6-4b36-96f0-38c212c05076 nodeName:}" failed. No retries permitted until 2025-02-26 13:51:20.918315555 +0000 UTC m=+2212.977397900 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/2645786c-50c6-4b36-96f0-38c212c05076-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-pl99z" (UID: "2645786c-50c6-4b36-96f0-38c212c05076") : secret "ingress-nginx-admission" not found
Feb 26 13:51:21 minikube kubelet[1437]: I0226 13:51:21.188084    1437 scope.go:117] "RemoveContainer" containerID="b7caa1cd2c91857306af307efc0094d7921eedc32b2e1261b6780d6cb79270df"
Feb 26 13:51:22 minikube kubelet[1437]: I0226 13:51:22.379394    1437 scope.go:117] "RemoveContainer" containerID="b7caa1cd2c91857306af307efc0094d7921eedc32b2e1261b6780d6cb79270df"
Feb 26 13:51:22 minikube kubelet[1437]: I0226 13:51:22.421234    1437 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-pl99z" podStartSLOduration=3.421211102 podStartE2EDuration="3.421211102s" podCreationTimestamp="2025-02-26 13:51:19 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-26 13:51:22.376363707 +0000 UTC m=+2214.435446077" watchObservedRunningTime="2025-02-26 13:51:22.421211102 +0000 UTC m=+2214.480293452"
Feb 26 13:51:22 minikube kubelet[1437]: I0226 13:51:22.642768    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lb9br\" (UniqueName: \"kubernetes.io/projected/4365767c-c243-459b-ac12-80e51f19a479-kube-api-access-lb9br\") pod \"4365767c-c243-459b-ac12-80e51f19a479\" (UID: \"4365767c-c243-459b-ac12-80e51f19a479\") "
Feb 26 13:51:22 minikube kubelet[1437]: I0226 13:51:22.650440    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4365767c-c243-459b-ac12-80e51f19a479-kube-api-access-lb9br" (OuterVolumeSpecName: "kube-api-access-lb9br") pod "4365767c-c243-459b-ac12-80e51f19a479" (UID: "4365767c-c243-459b-ac12-80e51f19a479"). InnerVolumeSpecName "kube-api-access-lb9br". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 26 13:51:22 minikube kubelet[1437]: I0226 13:51:22.743339    1437 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-lb9br\" (UniqueName: \"kubernetes.io/projected/4365767c-c243-459b-ac12-80e51f19a479-kube-api-access-lb9br\") on node \"minikube\" DevicePath \"\""
Feb 26 13:51:23 minikube kubelet[1437]: I0226 13:51:23.488027    1437 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3d6a698a638e595109aa4b6380d093b3a657cc2760bb45557d1ac4b356339a81"
Feb 26 13:51:23 minikube kubelet[1437]: I0226 13:51:23.750531    1437 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mfbdd\" (UniqueName: \"kubernetes.io/projected/8e3990c5-47a9-46a1-bdbe-8cd151c75b99-kube-api-access-mfbdd\") pod \"8e3990c5-47a9-46a1-bdbe-8cd151c75b99\" (UID: \"8e3990c5-47a9-46a1-bdbe-8cd151c75b99\") "
Feb 26 13:51:23 minikube kubelet[1437]: I0226 13:51:23.766343    1437 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8e3990c5-47a9-46a1-bdbe-8cd151c75b99-kube-api-access-mfbdd" (OuterVolumeSpecName: "kube-api-access-mfbdd") pod "8e3990c5-47a9-46a1-bdbe-8cd151c75b99" (UID: "8e3990c5-47a9-46a1-bdbe-8cd151c75b99"). InnerVolumeSpecName "kube-api-access-mfbdd". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 26 13:51:23 minikube kubelet[1437]: I0226 13:51:23.852037    1437 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-mfbdd\" (UniqueName: \"kubernetes.io/projected/8e3990c5-47a9-46a1-bdbe-8cd151c75b99-kube-api-access-mfbdd\") on node \"minikube\" DevicePath \"\""
Feb 26 13:51:24 minikube kubelet[1437]: I0226 13:51:24.593273    1437 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a1f06a688ceb2cb8fa829e2533aec1df9044a7972e3bcd36129b4280e883d282"
Feb 26 13:51:30 minikube kubelet[1437]: I0226 13:51:30.067501    1437 scope.go:117] "RemoveContainer" containerID="c6ea69e531ac8248e1e74f6d25e57c5fe6bdf808a6753d2be680013193979825"
Feb 26 13:51:30 minikube kubelet[1437]: I0226 13:51:30.121578    1437 scope.go:117] "RemoveContainer" containerID="ce35eb4829216cace19d952549181df0a18ce6398a1f5b8eea6a180ca8d7e150"


==> storage-provisioner [558d6fcf12f1] <==
I0226 13:14:34.479770       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0226 13:15:04.494094       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [d722a02367e8] <==
I0226 13:15:17.571188       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0226 13:15:17.584380       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0226 13:15:17.584996       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0226 13:15:35.027310       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0226 13:15:35.027462       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"45625363-d591-4e6c-aa14-635886cd34c8", APIVersion:"v1", ResourceVersion:"66762", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_9236e242-647a-44f1-bb8f-ab0b97afeff0 became leader
I0226 13:15:35.028179       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_9236e242-647a-44f1-bb8f-ab0b97afeff0!
I0226 13:15:35.129900       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9236e242-647a-44f1-bb8f-ab0b97afeff0!

